{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Sense Disambiguation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHEyAfuLfmDiyY9GfMqrzR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKrZ4kSnlQvw"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh5dGONOlVBk"
      },
      "source": [
        "!pip install wikipedia\r\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R288jN5ClcEI"
      },
      "source": [
        "import spacy\r\n",
        "import wikipedia\r\n",
        "import copy\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "nlp = spacy.load('en')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKGtt2nWkUql"
      },
      "source": [
        "def pages_to_sentences(*pages):    \r\n",
        "  \"\"\"Return a list of sentences in Wikipedia articles.\"\"\"    \r\n",
        "  sentences = []\r\n",
        "  for page in pages:\r\n",
        "      p = wikipedia.page(page)\r\n",
        "      document = nlp(p.content)\r\n",
        "      sentences += [sentence.text for sentence in document.sents]   \r\n",
        "  return sentences\r\n",
        "    \r\n",
        "def get_corpus_labels(pages_titles):\r\n",
        "  \"\"\"Return a tuple (corpus, labels) from a dictionary of Wikipedia articles' titles\"\"\"\r\n",
        "  corpus = []\r\n",
        "  labels = []\r\n",
        "  for k, v in pages_titles.items():\r\n",
        "    sentences = pages_to_sentences(*v)\r\n",
        "    corpus += sentences\r\n",
        "    labels += [k]*len(sentences)\r\n",
        "  return corpus, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDW8VygJqIct"
      },
      "source": [
        "from spacy.lang.en import STOP_WORDS\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "\r\n",
        "def lemmatizer(text):\r\n",
        "  return [word.lemma_ for word in nlp(text)]\r\n",
        "\r\n",
        "def get_stop_words_lemma(words=None):\r\n",
        "  \"\"\"Return a lemmatized set of the english stop words, extended, if specified, with the list of given words\"\"\"\r\n",
        "  stop_words = copy.deepcopy(STOP_WORDS)\r\n",
        "  if words != None:\r\n",
        "    assert isinstance(words, list), \"The passed parameter is not a list!\"\r\n",
        "    stop_words.update(words)\r\n",
        "  stop_words_str = \" \".join(stop_words)\r\n",
        "  return set(lemmatizer(stop_words_str))\r\n",
        "\r\n",
        "def get_classifier(corpus, labels, exclude_words=None):\r\n",
        "  stop_words_lemma = get_stop_words_lemma(exclude_words)\r\n",
        "  tfidf = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer=lemmatizer, ngram_range=(1, 2))\r\n",
        "  pipe = Pipeline([('vectorizer', tfidf), ('classifier', MultinomialNB())])\r\n",
        "  pipe.fit(corpus, labels)\r\n",
        "  print(\"Training accuracy: {:.2f}%\".format(pipe.score(corpus, labels)*100))\r\n",
        "  return pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAnW4RNq2R5g"
      },
      "source": [
        "def get_labels_indexes(classifier, labels):\r\n",
        "  \"\"\"Return a dictionary {index : label} of each label probability index in predict_proba() result array\"\"\"\r\n",
        "  dic = {}\r\n",
        "  y_proba = classifier.predict_proba(labels)\r\n",
        "  for i, label in enumerate(labels):\r\n",
        "    index = y_proba[i].argmax()\r\n",
        "    dic[index] = label\r\n",
        "  return dic\r\n",
        "\r\n",
        "def predict(classifier, labels, test_corpus):\r\n",
        "  predictions = []\r\n",
        "  class_labels = get_labels_indexes(classifier, labels)\r\n",
        "  y_proba = classifier.predict_proba(test_corpus)\r\n",
        "  for i in range(len(y_proba)):\r\n",
        "    max_index = y_proba[i].argmax()\r\n",
        "    predictions.append((class_labels[max_index], y_proba[i, max_index]))\r\n",
        "  return predictions\r\n",
        "\r\n",
        "def print_predictions(test_corpus, predictions):\r\n",
        "  for i in range(len(predictions)):\r\n",
        "    print(test_corpus[i], \"--> {} at {:g}%\".format(predictions[i][0], 100*predictions[i][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8q_LW80AEI6"
      },
      "source": [
        "# **Test Case : Amazon**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCGEiiT8_-P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114e0853-4c11-455a-e258-c1903166c575"
      },
      "source": [
        "amazon_pages_titles = {'greek': ['Amazons'],\r\n",
        "          'company': ['Amazon_(company)'],\r\n",
        "          'rainforest': ['Amazon_rainforest']}\r\n",
        "\r\n",
        "corpus, labels = get_corpus_labels(amazon_pages_titles)\r\n",
        "\r\n",
        "classifier = get_classifier(corpus, labels, ['amazon'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 85.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoDrp_uc12ho",
        "outputId": "2c00518f-6e92-43d5-e6e8-6c9352901a70"
      },
      "source": [
        "class_labels = ['greek', 'company', 'rainforest']\r\n",
        "\r\n",
        "test_corpus = [\"Amazon.com needed more than private investors to underwrite the expansion.\",\r\n",
        "               \"Bezos dismissed naysayers as not understanding the massive growth potential of the Internet.\",\r\n",
        "               \"As the decade ends, Amazon has set its sights on online advertising.\",\r\n",
        "               \"Finally, here was evidence of the women warriors that could have inspired the Amazon myths.\",\r\n",
        "               \"The triumph of patriarchy brings with it the liberation of the spirit from the manifestations of nature.\",\r\n",
        "               \"The creators of Wonder Woman had no interest in proving an actual link to the past.\",\r\n",
        "               \"The Amazon helps stabilize local and global climate.\",\r\n",
        "               \"The Amazon is a vast region that spans across eight rapidly developing countries.\",\r\n",
        "               \"Amazonia is the largest river basin in the world!\"]\r\n",
        "\r\n",
        "test_labels = ['company']*3 + ['greek']*3 + ['rainforest']*3\r\n",
        "\r\n",
        "predictions = predict(classifier, class_labels, test_corpus)\r\n",
        "\r\n",
        "print_predictions(test_corpus, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amazon.com needed more than private investors to underwrite the expansion. --> company at 63.5307%\n",
            "Bezos dismissed naysayers as not understanding the massive growth potential of the Internet. --> company at 75.4208%\n",
            "As the decade ends, Amazon has set its sights on online advertising. --> company at 58.5229%\n",
            "Finally, here was evidence of the women warriors that could have inspired the Amazon myths. --> greek at 69.0461%\n",
            "The triumph of patriarchy brings with it the liberation of the spirit from the manifestations of nature. --> company at 43.6495%\n",
            "The creators of Wonder Woman had no interest in proving an actual link to the past. --> greek at 53.836%\n",
            "The Amazon helps stabilize local and global climate. --> company at 55.165%\n",
            "The Amazon is a vast region that spans across eight rapidly developing countries. --> company at 48.2264%\n",
            "Amazonia is the largest river basin in the world! --> rainforest at 43.9912%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgALsUJzXzcd",
        "outputId": "be2f2428-538d-41bd-c727-d41b9e72c00a"
      },
      "source": [
        "print(\"Testing accuracy : {:.2f}%\".format(classifier.score(test_corpus, test_labels)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy : 66.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gbMsgbca7dm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\r\n",
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split( *shuffle(corpus, labels), test_size = 0.2, random_state = 12 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hktiEXeDbMoP",
        "outputId": "8cb0f42d-2b91-44fb-b64f-76e5e1e7e157"
      },
      "source": [
        "exclude_words =['amazon']\r\n",
        "stop_words_lemma = get_stop_words_lemma(exclude_words)\r\n",
        "tfidf = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer=lemmatizer, ngram_range=(1, 2))\r\n",
        "pipe = Pipeline([('vectorizer', tfidf), ('classifier', MultinomialNB())])\r\n",
        "\r\n",
        "param_grid = {'vectorizer__ngram_range' : [(1,1), (1,2)],\r\n",
        "              'vectorizer__tokenizer' : [None, lemmatizer]}\r\n",
        "\r\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\r\n",
        "\r\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  2.2min finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vectorizer',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 2),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words={\"'\", \"'d\"...\n",
              "                                                        use_idf=True,\n",
              "                                                        vocabulary=None)),\n",
              "                                       ('classifier',\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
              "                         'vectorizer__tokenizer': [None,\n",
              "                                                   <function lemmatizer at 0x7f37d173a560>]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFr5XnP8dSdh",
        "outputId": "2db53bbf-82dd-4c03-d2e3-c3a75367768b"
      },
      "source": [
        "print(grid_search.best_params_)\r\n",
        "print(grid_search.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None}\n",
            "0.7467579059261228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Vym_Z7dgRI",
        "outputId": "afcb1a29-3cae-4c45-e4af-f3c102cae339"
      },
      "source": [
        "grid_search.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([ 0.02382207, 10.94228411,  0.05049109, 10.92597303]),\n",
              " 'mean_score_time': array([0.00490932, 2.18896832, 0.00696607, 2.20551887]),\n",
              " 'mean_test_score': array([0.74675791, 0.73826291, 0.71850031, 0.68082647]),\n",
              " 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 2), (1, 2)],\n",
              "              mask=[False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_vectorizer__tokenizer': masked_array(data=[None, <function lemmatizer at 0x7f37d173a560>, None,\n",
              "                    <function lemmatizer at 0x7f37d173a560>],\n",
              "              mask=[False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None},\n",
              "  {'vectorizer__ngram_range': (1, 1),\n",
              "   'vectorizer__tokenizer': <function __main__.lemmatizer>},\n",
              "  {'vectorizer__ngram_range': (1, 2), 'vectorizer__tokenizer': None},\n",
              "  {'vectorizer__ngram_range': (1, 2),\n",
              "   'vectorizer__tokenizer': <function __main__.lemmatizer>}],\n",
              " 'rank_test_score': array([1, 2, 3, 4], dtype=int32),\n",
              " 'split0_test_score': array([0.72769953, 0.71361502, 0.70422535, 0.657277  ]),\n",
              " 'split1_test_score': array([0.70892019, 0.72769953, 0.68544601, 0.66666667]),\n",
              " 'split2_test_score': array([0.77830189, 0.78773585, 0.74528302, 0.71698113]),\n",
              " 'split3_test_score': array([0.75      , 0.71226415, 0.72169811, 0.66509434]),\n",
              " 'split4_test_score': array([0.76886792, 0.75      , 0.73584906, 0.69811321]),\n",
              " 'std_fit_time': array([0.00163064, 0.12581295, 0.00340832, 0.03621341]),\n",
              " 'std_score_time': array([0.00041951, 0.02529443, 0.00046384, 0.03878059]),\n",
              " 'std_test_score': array([0.02564879, 0.02821478, 0.0215633 , 0.02284105])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tqtrX-Lelht",
        "outputId": "8b194e83-0e46-40c6-8d80-37e4199a0711"
      },
      "source": [
        "estimator = grid_search.best_estimator_\r\n",
        "estimator.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7969924812030075"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdjYO2g0ezH2",
        "outputId": "4276b2e1-d56b-4f42-a29d-b00b6b7e22ce"
      },
      "source": [
        "estimator.score(test_corpus, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}