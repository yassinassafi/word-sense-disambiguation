{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Sense Disambiguation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "W-pPGiF0B7Fv",
        "D8q_LW80AEI6"
      ],
      "mount_file_id": "1i7AA8dPy2IMqUnVlOqC97wy6haGrZmvL",
      "authorship_tag": "ABX9TyO/U7jOqfKER/2Eirfyzi02"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKrZ4kSnlQvw"
      },
      "source": [
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh5dGONOlVBk"
      },
      "source": [
        "!pip install wikipedia\r\n",
        "!pip install praw\r\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R288jN5ClcEI"
      },
      "source": [
        "import spacy\r\n",
        "import praw\r\n",
        "import wikipedia\r\n",
        "import copy\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "nlp = spacy.load('en')   "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-pPGiF0B7Fv"
      },
      "source": [
        "# **Wikipedia Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKGtt2nWkUql"
      },
      "source": [
        "def pages_to_sentences(*pages):    \r\n",
        "  \"\"\"Return a list of sentences in Wikipedia articles.\"\"\"    \r\n",
        "  sentences = []\r\n",
        "  for page in pages:\r\n",
        "      p = wikipedia.page(page)\r\n",
        "      document = nlp(p.content)\r\n",
        "      sentences += [sentence.text for sentence in document.sents]   \r\n",
        "  return sentences\r\n",
        "    \r\n",
        "def get_corpus_labels(pages_titles):\r\n",
        "  \"\"\"Return a tuple (corpus, labels) from a dictionary of Wikipedia articles' titles\"\"\"\r\n",
        "  corpus = []\r\n",
        "  labels = []\r\n",
        "  for k, v in pages_titles.items():\r\n",
        "    sentences = pages_to_sentences(*v)\r\n",
        "    corpus += sentences\r\n",
        "    labels += [k]*len(sentences)\r\n",
        "  return corpus, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDW8VygJqIct"
      },
      "source": [
        "from spacy.lang.en import STOP_WORDS\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "\r\n",
        "def lemmatizer(text):\r\n",
        "  return [word.lemma_ for word in nlp(text)]\r\n",
        "\r\n",
        "def get_stop_words_lemma(words=None):\r\n",
        "  \"\"\"Return a lemmatized set of the english stop words, extended, if specified, with the list of given words\"\"\"\r\n",
        "  stop_words = copy.deepcopy(STOP_WORDS)\r\n",
        "  if words != None:\r\n",
        "    assert isinstance(words, list), \"The passed parameter is not a list!\"\r\n",
        "    stop_words.update(words)\r\n",
        "  stop_words_str = \" \".join(stop_words)\r\n",
        "  return set(lemmatizer(stop_words_str))\r\n",
        "\r\n",
        "def get_classifier(corpus, labels, exclude_words=None):\r\n",
        "  stop_words_lemma = get_stop_words_lemma(exclude_words)\r\n",
        "  tfidf = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer=lemmatizer, ngram_range=(1, 2))\r\n",
        "  pipe = Pipeline([('vectorizer', tfidf), ('classifier', MultinomialNB())])\r\n",
        "  pipe.fit(corpus, labels)\r\n",
        "  print(\"Training accuracy: {:.2f}%\".format(pipe.score(corpus, labels)*100))\r\n",
        "  return pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAnW4RNq2R5g"
      },
      "source": [
        "def get_labels_indexes(classifier, labels):\r\n",
        "  \"\"\"Return a dictionary {index : label} of each label probability index in predict_proba() result array\"\"\"\r\n",
        "  dic = {}\r\n",
        "  y_proba = classifier.predict_proba(labels)\r\n",
        "  for i, label in enumerate(labels):\r\n",
        "    index = y_proba[i].argmax()\r\n",
        "    dic[index] = label\r\n",
        "  return dic\r\n",
        "\r\n",
        "def predict(classifier, labels, test_corpus):\r\n",
        "  predictions = []\r\n",
        "  class_labels = get_labels_indexes(classifier, labels)\r\n",
        "  y_proba = classifier.predict_proba(test_corpus)\r\n",
        "  for i in range(len(y_proba)):\r\n",
        "    max_index = y_proba[i].argmax()\r\n",
        "    predictions.append((class_labels[max_index], y_proba[i, max_index]))\r\n",
        "  return predictions\r\n",
        "\r\n",
        "def print_predictions(test_corpus, predictions):\r\n",
        "  for i in range(len(predictions)):\r\n",
        "    print(test_corpus[i], \"--> {} at {:g}%\".format(predictions[i][0], 100*predictions[i][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8q_LW80AEI6"
      },
      "source": [
        "## **Test Case : Amazon**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCGEiiT8_-P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4c5def-7f2a-4736-e345-ea61c680501f"
      },
      "source": [
        "amazon_pages_titles = {'greek': ['Amazons'],\r\n",
        "          'company': ['Amazon_(company)'],\r\n",
        "          'rainforest': ['Amazon_rainforest']}\r\n",
        "\r\n",
        "corpus, labels = get_corpus_labels(amazon_pages_titles)\r\n",
        "\r\n",
        "classifier = get_classifier(corpus, labels, ['amazon'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 85.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoDrp_uc12ho",
        "outputId": "f9102758-8761-400f-9dcf-755b2cb1fb13"
      },
      "source": [
        "class_labels = ['greek', 'company', 'rainforest']\r\n",
        "\r\n",
        "test_corpus = [\"Amazon.com needed more than private investors to underwrite the expansion.\",\r\n",
        "               \"Bezos dismissed naysayers as not understanding the massive growth potential of the Internet.\",\r\n",
        "               \"As the decade ends, Amazon has set its sights on online advertising.\",\r\n",
        "               \"Finally, here was evidence of the women warriors that could have inspired the Amazon myths.\",\r\n",
        "               \"The triumph of patriarchy brings with it the liberation of the spirit from the manifestations of nature.\",\r\n",
        "               \"The creators of Wonder Woman had no interest in proving an actual link to the past.\",\r\n",
        "               \"The Amazon helps stabilize local and global climate.\",\r\n",
        "               \"The Amazon is a vast region that spans across eight rapidly developing countries.\",\r\n",
        "               \"Amazonia is the largest river basin in the world!\"]\r\n",
        "\r\n",
        "test_labels = ['company']*3 + ['greek']*3 + ['rainforest']*3\r\n",
        "\r\n",
        "predictions = predict(classifier, class_labels, test_corpus)\r\n",
        "\r\n",
        "print_predictions(test_corpus, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amazon.com needed more than private investors to underwrite the expansion. --> company at 63.5299%\n",
            "Bezos dismissed naysayers as not understanding the massive growth potential of the Internet. --> company at 75.4074%\n",
            "As the decade ends, Amazon has set its sights on online advertising. --> company at 58.5215%\n",
            "Finally, here was evidence of the women warriors that could have inspired the Amazon myths. --> greek at 69.0468%\n",
            "The triumph of patriarchy brings with it the liberation of the spirit from the manifestations of nature. --> company at 43.6488%\n",
            "The creators of Wonder Woman had no interest in proving an actual link to the past. --> greek at 53.8367%\n",
            "The Amazon helps stabilize local and global climate. --> company at 55.1641%\n",
            "The Amazon is a vast region that spans across eight rapidly developing countries. --> company at 48.2254%\n",
            "Amazonia is the largest river basin in the world! --> rainforest at 43.9917%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgALsUJzXzcd",
        "outputId": "79384476-1dbb-4c49-d1c5-47f5a188f567"
      },
      "source": [
        "print(\"Testing accuracy : {:.2f}%\".format(classifier.score(test_corpus, test_labels)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy : 66.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gbMsgbca7dm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\r\n",
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split( *shuffle(corpus, labels), test_size = 0.2, random_state = 12 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hktiEXeDbMoP",
        "outputId": "3622963e-8399-4e26-bb74-54b58128b4f5"
      },
      "source": [
        "exclude_words =['amazon']\r\n",
        "stop_words_lemma = get_stop_words_lemma(exclude_words)\r\n",
        "tfidf = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer=lemmatizer, ngram_range=(1, 2))\r\n",
        "pipe = Pipeline([('vectorizer', tfidf), ('classifier', MultinomialNB())])\r\n",
        "\r\n",
        "param_grid = {'vectorizer__ngram_range' : [(1,1), (1,2)],\r\n",
        "              'vectorizer__tokenizer' : [None, lemmatizer]}\r\n",
        "\r\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\r\n",
        "\r\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vectorizer',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 2),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words={\"'\", \"'s\"...\n",
              "                                                        use_idf=True,\n",
              "                                                        vocabulary=None)),\n",
              "                                       ('classifier',\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
              "                         'vectorizer__tokenizer': [None,\n",
              "                                                   <function lemmatizer at 0x7f4af3b32710>]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFr5XnP8dSdh",
        "outputId": "722afe25-bb62-4075-9d0a-dafabe95a093"
      },
      "source": [
        "print(grid_search.best_params_)\r\n",
        "print(grid_search.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None}\n",
            "0.7476614403401541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Vym_Z7dgRI",
        "outputId": "4aa47ac1-e439-4259-b0e6-7b68e5717289"
      },
      "source": [
        "grid_search.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.02230062, 9.53248277, 0.04151025, 9.57963772]),\n",
              " 'mean_score_time': array([0.00487766, 1.92911968, 0.00805893, 1.94374962]),\n",
              " 'mean_test_score': array([0.74766144, 0.73349721, 0.71375233, 0.68456462]),\n",
              " 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 2), (1, 2)],\n",
              "              mask=[False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_vectorizer__tokenizer': masked_array(data=[None, <function lemmatizer at 0x7f4af3b32710>, None,\n",
              "                    <function lemmatizer at 0x7f4af3b32710>],\n",
              "              mask=[False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None},\n",
              "  {'vectorizer__ngram_range': (1, 1),\n",
              "   'vectorizer__tokenizer': <function __main__.lemmatizer>},\n",
              "  {'vectorizer__ngram_range': (1, 2), 'vectorizer__tokenizer': None},\n",
              "  {'vectorizer__ngram_range': (1, 2),\n",
              "   'vectorizer__tokenizer': <function __main__.lemmatizer>}],\n",
              " 'rank_test_score': array([1, 2, 3, 4], dtype=int32),\n",
              " 'split0_test_score': array([0.75117371, 0.75586854, 0.71830986, 0.68544601]),\n",
              " 'split1_test_score': array([0.72769953, 0.7370892 , 0.70422535, 0.67605634]),\n",
              " 'split2_test_score': array([0.75      , 0.71698113, 0.72641509, 0.66509434]),\n",
              " 'split3_test_score': array([0.72641509, 0.70283019, 0.67924528, 0.67924528]),\n",
              " 'split4_test_score': array([0.78301887, 0.75471698, 0.74056604, 0.71698113]),\n",
              " 'std_fit_time': array([0.00142012, 0.04435993, 0.00268677, 0.15866447]),\n",
              " 'std_score_time': array([0.00055296, 0.04597292, 0.00408754, 0.0490111 ]),\n",
              " 'std_test_score': array([0.02058076, 0.02086578, 0.02088823, 0.01750077])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tqtrX-Lelht",
        "outputId": "29880325-fcdf-4647-fec9-a497569a1afb"
      },
      "source": [
        "estimator = grid_search.best_estimator_\r\n",
        "estimator.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7781954887218046"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdjYO2g0ezH2",
        "outputId": "6ab24524-18f1-4b36-99e1-287fde005e85"
      },
      "source": [
        "estimator.score(test_corpus, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okxLzbkMB0KC"
      },
      "source": [
        "# **Reddit Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEqtSVTCDpF-"
      },
      "source": [
        "def create_reddit_connection(json_file_path = '/content/drive/MyDrive/reddit_config.json', api_key = 'homonyms'):\r\n",
        "  '''Returns a reddit object connected to the api credentials provided by the json configuration file'''\r\n",
        "  \r\n",
        "  with open(json_file_path, 'r') as f:\r\n",
        "    data = json.load(f)\r\n",
        "\r\n",
        "  user_values = data[api_key]\r\n",
        "\r\n",
        "  reddit = praw.Reddit(client_id=user_values['client_id'],\r\n",
        "                       client_secret=user_values['client_secret'],\r\n",
        "                       password=user_values['password'],\r\n",
        "                       user_agent=user_values['user_agent'],\r\n",
        "                       username=user_values['username'],\r\n",
        "                       check_for_async=False)\r\n",
        "  \r\n",
        "  return reddit\r\n",
        "\r\n",
        "reddit = create_reddit_connection()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1TaPPYFKfoj"
      },
      "source": [
        "def fetch_subreddit(reddit_connection, topic):\r\n",
        "  return reddit_connection.subreddit(topic)\r\n",
        "\r\n",
        "amazon_subred = fetch_subreddit(reddit, \"amazon\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjY6aOfHIx0Q"
      },
      "source": [
        "def fetch_hot_content(subreddit, limit=11):\r\n",
        "  hot = subreddit.hot(limit=limit)\r\n",
        "  content = {}\r\n",
        "  for i in range(limit):\r\n",
        "    submission = next(hot)\r\n",
        "    content[submission.title] = submission.selftext\r\n",
        "  return content\r\n",
        "\r\n",
        "amazon_hot = fetch_hot_content(amazon_subred)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EXBQs58Ir4R",
        "outputId": "7a2d6580-4ca1-47d3-9fc5-0bd721da0bce"
      },
      "source": [
        "amazon_hot"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"Amazon Prime Video Sets 'Them' Premiere Date (TV News Roundup)\": '',\n",
              " 'Amazon Quietly Began Building a Grocery Chain During Pandemic': '',\n",
              " 'Amazon workers voting to unionize': '[1,000 Amazon staff ask about unionization after Alabama union vote (businessinsider.com)](https://www.businessinsider.com/alabama-amazon-union-vote-other-staff-unionization-rwdsu-2021-3) \\n\\nI canceled my prime account today and emailed Jeff Bezos in solidarity with these workers because God knows he can afford to treat his employees humanely.',\n",
              " 'Amazon-Backed Rivian Seeks 3-in-a-Row EV Debuts, Defying History': '',\n",
              " 'How Coupang is “out-Amazoning even Amazon,” according to Goodwater Capital': '',\n",
              " 'MacKenzie Scott, a Philanthropist and Ex-Wife of Jeff Bezos, Remarries': '',\n",
              " 'PS5 restock: Latest updates at Amazon, Best Buy, GameStop, Sony, Walmart': '',\n",
              " 'Review Thursday - All about Reviews': 'This thread is a place for questions and general discussion about reviews at Amazon. (This thread is **not** for posting reviews of your own.)',\n",
              " 'The best AirPods cases, from Chanel to Amazon': '',\n",
              " \"There's a hot deal happening on Amazon's No. 1 best-selling fire pit today\": '',\n",
              " 'Weekly Help and Discussion Thread for the week of December 28, 2020': \"This thread is a place for Amazon questions and general discussion. This is also the *only* place you may ask individual questions about delivery, shipping, returns, and account issues.\\n\\nIf you want to complain about something, please use the Meltdown Monday thread instead.\\n\\n*Make a top-level comment if you want to ask a question or start a discussion! Also, please don't downvote questions!*\\n\\n**A big thank you to the many people who take time to answer other people's questions!**\\n\\nFor past help threads, please search the [Weekly Archive](https://www.reddit.com/r/amazon/search?q=title%3A%28%22Weekly+Help%22%29+author%3AAutoModerator&restrict_sr=on&sort=new&t=year).\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}